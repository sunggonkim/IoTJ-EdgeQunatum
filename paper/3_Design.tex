\section{Design of \EdgeQuantum}\label{sec:design}

This section presents the architecture and key design decisions of \EdgeQuantum, a quantum circuit simulator optimized for edge devices with unified memory architecture. We first provide an overview of the system architecture (\S\ref{sec:overview}), then detail the critical breakthrough in unified virtual memory (UVM) utilization (\S\ref{sec:uvm}), the zero-copy I/O pipeline (\S\ref{sec:pipeline}), and edge-specific optimizations (\S\ref{sec:optimizations}).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/3.1-design.pdf}
    \vspace{-.2cm}
    \caption{Overall architecture of \EdgeQuantum. The system exploits CUDA Unified Virtual Memory (UVM) to enable direct disk-to-GPU data flow without explicit memory copies, achieving 2$\times$ speedup over conventional pinned memory approaches.}
    \label{fig:architecture}
    \vspace{-.3cm}
\end{figure}

\subsection{System Overview}\label{sec:overview}

\noindent\textbf{Design Goals.} \EdgeQuantum is designed with three primary objectives:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Scalability beyond RAM:} Enable simulation of quantum circuits whose state vector exceeds the physical memory capacity of the edge device.
    \item \textbf{True zero-copy operation:} Eliminate explicit \texttt{cudaMemcpy} calls by leveraging unified memory, reducing data movement overhead.
    \item \textbf{Maximize hardware utilization:} Exploit the shared CPU-GPU memory bus of embedded SoCs (e.g., NVIDIA Jetson) to achieve near-peak throughput.
\end{enumerate}

\noindent\textbf{Key Insight: UVM Compatibility Discovery.} A critical contribution of this work is the discovery that \textit{CUDA Unified Virtual Memory (UVM) is fully compatible with POSIX file I/O on Jetson's unified memory architecture}. Contrary to conventional assumptions that \texttt{cudaMallocManaged} pointers cannot be used with system calls like \texttt{pread()}, our extensive testing reveals that:

\begin{enumerate}[leftmargin=*]
    \item UVM pointers can be directly passed to \texttt{pread()}/\texttt{pwrite()} system calls.
    \item UVM is compatible with \texttt{O\_DIRECT} flag for bypassing the page cache.
    \item cuStateVec library fully supports UVM memory for quantum gate operations.
\end{enumerate}

This discovery enables a fundamentally simpler and faster architecture than previously believed possible.

\noindent\textbf{Architectural Overview.} Figure~\ref{fig:architecture} illustrates the overall architecture. \EdgeQuantum partitions the full quantum state vector into fixed-size chunks (256\,MB by default) and processes them through a streamlined two-stage pipeline:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Load Stage:} Read the chunk directly from NVMe storage into UVM buffer using \texttt{pread()} with \texttt{O\_DIRECT}.
    \item \textbf{Compute Stage:} Execute quantum gate operations on the same UVM buffer using cuStateVec---no memory copy required.
    \item \textbf{Store Stage:} Write the processed chunk directly from UVM buffer to NVMe using \texttt{pwrite()} with \texttt{O\_DIRECT}.
\end{enumerate}

The elimination of \texttt{cudaMemcpy} calls provides a \textbf{2$\times$ performance improvement} over conventional pinned memory approaches.

\subsection{Unified Virtual Memory Architecture}\label{sec:uvm}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/memory-architecture.pdf}
    \vspace{-.2cm}
    \caption{Memory hierarchy comparison. (Left) Conventional approach requires explicit cudaMemcpy between pinned host memory and GPU device memory. (Right) \EdgeQuantum's UVM approach enables direct disk-GPU data flow without memory copies.}
    \label{fig:memory}
    \vspace{-.3cm}
\end{figure}

\noindent\textbf{UVM Compatibility Analysis.} Table~\ref{tab:memory_compat} summarizes the compatibility of different CUDA memory types with cuStateVec and POSIX I/O on Jetson Orin:

\begin{table}[t]
\centering
\caption{Memory type compatibility on Jetson Orin Nano. UVM is the only type compatible with both cuStateVec and direct disk I/O.}
\label{tab:memory_compat}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Memory Type} & \textbf{cuStateVec} & \textbf{pread/pwrite} & \textbf{O\_DIRECT} \\
\midrule
\texttt{cudaMalloc} (Device) & \cmark & \xmark & \xmark \\
\texttt{cudaMallocHost} (Pinned) & \xmark & \cmark & \cmark \\
\texttt{cudaMallocManaged} (UVM) & \cmark & \cmark & \cmark \\
\bottomrule
\end{tabular}
\vspace{-.3cm}
\end{table}

The key findings are:
\begin{itemize}[leftmargin=*]
    \item \textbf{Device Memory:} Requires explicit \texttt{cudaMemcpy} for disk I/O, as device addresses are not accessible to kernel syscalls.
    \item \textbf{Pinned Host Memory:} Rejected by cuStateVec's \texttt{custatevecApplyMatrix} with error code \texttt{CUSTATEVEC\_STATUS\_INVALID\_VALUE} when passed via \texttt{cudaHostGetDevicePointer}.
    \item \textbf{UVM (Managed Memory):} Fully compatible with both POSIX syscalls and cuStateVec operations.
\end{itemize}

\noindent\textbf{Why UVM Works on Jetson.} Jetson SoCs feature a true Unified Memory Architecture (UMA) where CPU and GPU share the same physical DRAM:

\begin{itemize}[leftmargin=*]
    \item \textbf{Shared Physical Memory:} Both CPU and GPU access the same LPDDR5 banks, eliminating the need for PCIe-based data transfers.
    \item \textbf{Coherent Memory Bus:} The memory controller maintains cache coherency between CPU and GPU views of UVM pages.
    \item \textbf{Page Migration:} UVM pages can reside in CPU-accessible or GPU-accessible regions; the driver migrates pages transparently based on access patterns.
\end{itemize}

When \texttt{pread()} is called with a UVM pointer, the kernel accesses the physical pages backing the UVM allocation. After the I/O completes, \texttt{cudaDeviceSynchronize()} ensures GPU visibility of the updated data before cuStateVec kernel execution.

\noindent\textbf{Performance Advantage.} The elimination of explicit memory copies provides substantial performance benefits:

\begin{table}[t]
\centering
\caption{Performance comparison: Pinned+cudaMemcpy vs. UVM direct access. Measurements include full round-trip (read, compute, write).}
\label{tab:uvm_perf}
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Qubits} & \textbf{Pinned+Copy} & \textbf{UVM Direct} & \textbf{Speedup} \\
\midrule
20 (16 MB) & 18.9 ms & 9.8 ms & 1.92$\times$ \\
22 (64 MB) & 74.6 ms & 37.0 ms & 2.01$\times$ \\
24 (256 MB) & 298 ms & 149 ms & 2.00$\times$ \\
\bottomrule
\end{tabular}
\vspace{-.3cm}
\end{table}

The consistent 2$\times$ speedup across different state vector sizes demonstrates that memory copy overhead is a significant bottleneck in conventional approaches. By eliminating this overhead, \EdgeQuantum achieves near-theoretical I/O bandwidth utilization.

\noindent\textbf{Data Flow Comparison.} The conventional approach requires four data movements per chunk:
\[
\text{Disk} \xrightarrow{\texttt{pread}} \text{Pinned} \xrightarrow{\texttt{cudaMemcpy}} \text{GPU} \xrightarrow{\texttt{cudaMemcpy}} \text{Pinned} \xrightarrow{\texttt{pwrite}} \text{Disk}
\]

\EdgeQuantum's UVM approach requires only two:
\[
\text{Disk} \xrightarrow{\texttt{pread}} \text{UVM} \xrightarrow[\text{cuStateVec}]{\text{in-place}} \text{UVM} \xrightarrow{\texttt{pwrite}} \text{Disk}
\]

This 50\% reduction in data movements directly translates to the observed 2$\times$ speedup.

\subsection{Zero-Copy I/O Pipeline}\label{sec:pipeline}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/3.2-design.pdf}
    \vspace{-.2cm}
    \caption{UVM-based asynchronous pipeline. Each UVM buffer cycles through Load, Compute, and Store stages without memory copies.}
    \label{fig:pipeline}
    \vspace{-.3cm}
\end{figure}

\noindent\textbf{O\_DIRECT for Page Cache Bypass.} Standard Linux file I/O involves double-buffering through the page cache: data is first copied to kernel pages, then to user-space buffers. For quantum simulation where the state vector (e.g., 4\,GB for 28 qubits) exceeds available RAM, this page cache pollution evicts critical data and triggers excessive swapping. \EdgeQuantum opens state vector files with the \texttt{O\_DIRECT} flag, instructing the kernel to perform DMA directly from the NVMe device to UVM buffers. Our experiments confirm that:

\begin{lstlisting}[language=C++, basicstyle=\ttfamily\footnotesize]
void* uvm_ptr;
cudaMallocManaged(&uvm_ptr, size, cudaMemAttachGlobal);
int fd = open(path, O_RDONLY | O_DIRECT);
pread(fd, uvm_ptr, size, offset);  // Works!
\end{lstlisting}

The combination of UVM and O\_DIRECT provides:
\begin{itemize}[leftmargin=*]
    \item \textbf{No page cache overhead:} Data flows directly from NVMe to UVM.
    \item \textbf{Predictable latency:} No page fault handling or cache eviction.
    \item \textbf{Reduced memory footprint:} No kernel buffer copies.
\end{itemize}

\noindent\textbf{Triple-Buffer Orchestration.} As shown in Figure~\ref{fig:pipeline}, \EdgeQuantum maintains three UVM buffers (A, B, C) that cycle through the pipeline stages:

\begin{itemize}[leftmargin=*]
    \item \textbf{Time $t$:} Buffer A is loading $Chunk_{i+1}$, Buffer B is computing $Chunk_i$, Buffer C is storing $Chunk_{i-1}$.
    \item \textbf{Time $t+1$:} Buffers rotate: A $\rightarrow$ Compute, B $\rightarrow$ Store, C $\rightarrow$ Load.
\end{itemize}

Unlike conventional approaches that require six operations per chunk (pread $\rightarrow$ H2D copy $\rightarrow$ compute $\rightarrow$ D2H copy $\rightarrow$ pwrite), the UVM approach requires only three (pread $\rightarrow$ compute $\rightarrow$ pwrite), simplifying the pipeline and reducing synchronization overhead.

\noindent\textbf{Synchronization Strategy.} The critical synchronization point is ensuring GPU visibility of disk-loaded data:

\begin{lstlisting}[language=C++, basicstyle=\ttfamily\footnotesize]
// Load chunk to UVM (CPU operation)
pread(fd, uvm_ptr, chunk_size, offset);

// Ensure GPU sees the updated data
cudaDeviceSynchronize();

// Execute cuStateVec kernel (in-place on UVM)
custatevecApplyMatrix(handle, uvm_ptr, ...);

// Ensure computation complete before write
cudaDeviceSynchronize();

// Store chunk from UVM (CPU operation)
pwrite(fd, uvm_ptr, chunk_size, offset);
\end{lstlisting}

The \texttt{cudaDeviceSynchronize()} calls ensure that GPU-side caching of UVM pages is properly managed, maintaining data coherency between CPU I/O operations and GPU compute operations.

\noindent\textbf{Buffer State Machine.} Each buffer cycles through four states:
\[
\texttt{EMPTY} \rightarrow \texttt{LOADING} \rightarrow \texttt{READY} \rightarrow \texttt{STORING} \rightarrow \texttt{EMPTY}
\]

The orchestrator assigns operations based on buffer state, preventing race conditions where a buffer is overwritten before its contents are committed to storage.

\subsection{Edge-Specific Optimizations}\label{sec:optimizations}

\noindent\textbf{High-Priority CUDA Streams.} On resource-constrained edge GPUs, kernel launch latency can bottleneck performance. \EdgeQuantum creates CUDA streams with the highest available priority:

\begin{lstlisting}[language=C++, basicstyle=\ttfamily\footnotesize]
int leastPri, greatestPri;
cudaDeviceGetStreamPriorityRange(&leastPri, &greatestPri);
cudaStreamCreateWithPriority(&stream, 
    cudaStreamNonBlocking, greatestPri);
\end{lstlisting}

This ensures cuStateVec kernels preempt background GPU tasks and execute with minimal queuing delay.

\noindent\textbf{System-Level Performance Tuning.} To maximize throughput on Jetson Orin, \EdgeQuantum applies system-level optimizations via \texttt{jetson\_clocks}:
\begin{itemize}[leftmargin=*]
    \item \textbf{CPU Frequency:} Lock all cores at maximum (1.51\,GHz on Orin Nano).
    \item \textbf{GPU Frequency:} Lock at peak (625\,MHz on Orin Nano).
    \item \textbf{Memory Controller:} Lock EMC at maximum (2133\,MHz).
    \item \textbf{CPU Idle States:} Disable C7 and WFI states to eliminate wake-up latency.
\end{itemize}

These tunings reduce pipeline stalls by up to 20\% compared to default power-saving configurations.

\noindent\textbf{Memory Alignment.} O\_DIRECT imposes strict alignment constraints: both the buffer address and the I/O offset must align to the filesystem block size (4\,KB). \texttt{cudaMallocManaged} returns page-aligned pointers by default, and chunk sizes are multiples of 4\,KB, ensuring compliance without additional buffering.

\noindent\textbf{Graceful Fallback.} When O\_DIRECT initialization fails (e.g., on filesystems that do not support it), \EdgeQuantum gracefully falls back to buffered I/O with a warning:

\begin{lstlisting}[language=C++, basicstyle=\ttfamily\footnotesize]
fd = open(path, O_RDONLY | O_DIRECT);
if (fd < 0 && errno == EINVAL) {
    fd = open(path, O_RDONLY);  // Fallback
    use_odirect = false;
}
\end{lstlisting}

\subsection{Quantum Gate Execution}\label{sec:gates}

\noindent\textbf{Chunk-Local vs. Global Gates.} Quantum gates are classified by their target qubit index relative to the chunk size. For a chunk containing $2^{25}$ amplitudes (chunk\_bits = 25):
\begin{itemize}[leftmargin=*]
    \item \textbf{Chunk-local gates} (target qubit $< 25$): The gate affects only amplitudes within a single chunk. \EdgeQuantum applies these using cuStateVec's \texttt{custatevecApplyMatrix} directly on the UVM buffer.
    \item \textbf{Global gates} (target qubit $\geq 25$): The gate couples amplitudes across different chunks, requiring coordination between multiple chunks in memory.
\end{itemize}

\noindent\textbf{Global Gate Strategy.} For global gates, \EdgeQuantum loads the required pair of chunks into separate UVM buffers and performs a fused operation:
\begin{enumerate}[leftmargin=*]
    \item Load $Chunk_i$ and $Chunk_{i + 2^k}$ (where $k$ is the target qubit offset within the global range).
    \item Execute the gate operation treating the two buffers as a combined state vector.
    \item Write both chunks back to storage.
\end{enumerate}

This approach minimizes I/O overhead by batching chunk pairs rather than processing them individually.

\noindent\textbf{Gate Fusion.} Consecutive single-qubit gates targeting the same qubit are fused into a single unitary matrix:
\[
U_{\text{fused}} = U_n \times U_{n-1} \times \cdots \times U_1
\]

This reduces the number of kernel launches and memory passes, improving arithmetic intensity on the GPU.

\subsection{Comparison with Related Approaches}\label{sec:comparison}

Table~\ref{tab:comparison} summarizes how \EdgeQuantum differs from existing quantum simulators in its approach to memory management and I/O optimization.

\begin{table}[t]
\centering
\caption{Comparison of memory management strategies.}
\label{tab:comparison}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Approach} & \textbf{Memory Type} & \textbf{cudaMemcpy} & \textbf{Speedup} \\
\midrule
cuQuantum Native & Device & N/A & 1.0$\times$ \\
BMQSim~\cite{bmqsim} & Pinned+Device & Required & 1.0$\times$ \\
SV-Sim~\cite{svsim} & Mmap & N/A & 0.8$\times$ \\
\EdgeQuantum & \textbf{UVM} & \textbf{None} & \textbf{2.0$\times$} \\
\bottomrule
\end{tabular}
\vspace{-.3cm}
\end{table}

\noindent\textbf{Key Differentiators:}
\begin{enumerate}[leftmargin=*]
    \item \textbf{UVM-based Zero-Copy:} \EdgeQuantum is the first to demonstrate that UVM is compatible with POSIX I/O and cuStateVec on Jetson platforms, enabling true zero-copy operation.
    \item \textbf{2$\times$ Performance Gain:} By eliminating \texttt{cudaMemcpy} overhead, \EdgeQuantum achieves consistent 2$\times$ speedup over conventional pinned memory approaches.
    \item \textbf{Simplified Architecture:} The UVM approach reduces pipeline complexity from six operations to three, lowering synchronization overhead and code complexity.
    \item \textbf{Edge-First Design:} System-level tunings (jetson\_clocks, high-priority streams) are integrated into the runtime for optimal edge device performance.
\end{enumerate}
