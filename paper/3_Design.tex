\section{\EdgeQuantum Design}

In this section, we present the design of \EdgeQuantum, a scalable quantum circuit simulator optimized for edge devices with limited memory. 
\EdgeQuantum does not store the full state vector in DRAM, but instead partitions the state vector across NVMe storage and allocates Unified Virtual Memory (UVM) buffers to act as a staging area. 
This allows for the execution of large-scale quantum simulations that exceed physical RAM capacity. 
To overcome the high latency associated with disk I/O, \EdgeQuantum integrates a pipelined execution model that dynamically manages memory access permissions between the CPU and GPU, enabling efficient asynchronous overlap of computation and data transfer.

\subsection{Overall Procedure}~\label{design_1}

Figure~\ref{overall_architecture} shows the overall procedure of \EdgeQuantum.
\EdgeQuantum provides two main phases to support large-scale quantum circuit simulation on edge devices: \textit{Initialization} and \textit{Execution} phase.

\noindent\textbf{Initialization.} 
When quantum circuit simulation starts, \textit{Resource Manager} queries the available hardware resources on the edge device.
It reads metadata including the GPU memory capacity, unified memory size, NVMe storage bandwidth, and CPU core count (\ding{182}).
Once the metadata collection is completed, \textit{Chunk Calculator} determines the optimal partitioning strategy based on the number of input qubits (e.g., $n = 30$, which corresponds to $2^{30}$ amplitudes and a total memory size of 16 GB).
This calculation ensures that 1) the size of each state vector chunk fits within the available UVM buffer space while maximizing GPU occupancy and 2) the total number of chunks is aligned with the NVMe block size to facilitate efficient I/O operations (\ding{183}).

After completing the chunk configuration, \textit{Buffer Allocator} allocates the necessary intermediate buffers using CUDA Unified Virtual Memory.
Unlike standard device allocations, \textit{Buffer Allocator} initializes these buffers with the \texttt{cudaMemAttachHost} flag.
This allows the buffers to be immediately accessible by the CPU for file I/O operations before GPU kernel execution begins, establishing the foundation for the pipeline (\ding{184}).
Finally, \textit{State Initializer} generates the initial quantum state $|0\rangle^{\otimes n}$ and writes it to the NVMe storage, preparing the system for iterative simulation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{Figures/Architecture_EdgeQuantum.pdf}
    \caption{Overall procedure of \EdgeQuantum.}
    \label{overall_architecture}
\end{figure}

\noindent\textbf{Execution.}
After \textit{Initialization} phase is completed, \EdgeQuantum enters \textit{Execution} phase. 
First, \EdgeQuantum constructs a sequence of executable tasks, where each task corresponds to a specific quantum gate or circuit layer.
For each task, the system processes the full state vector by iterating through the stored chunks.
\EdgeQuantum employs a triple-buffer asynchronous pipeline to process these chunks.
This pipeline coordinates three distinct operations: loading data from NVMe, executing quantum kernels on the GPU, and storing results back to NVMe (\ding{185}).
To enable true concurrency between these operations without data races, \textit{Pipeline Coordinator} dynamically switches the access mode of UVM buffers.
It utilizes \texttt{cudaStreamAttachMemAsync} to toggle buffers between \texttt{AttachHost} mode for I/O operations and \texttt{AttachGlobal} mode for GPU computation (\ding{186}).
By manipulating page table permissions asynchronously, \EdgeQuantum eliminates the need for explicit memory copies between host and device.
Finally, \textit{Kernel Simulator} executes the quantum gate logic on the chunks currently in \texttt{AttachGlobal} mode.
Once the computation for a layer is complete, the system synchronizes the pipeline and proceeds to the next circuit depth (\ding{187}).

\subsection{Unified Memory Architecture Analysis}~\label{design_2}

Before defining the memory management strategy, \EdgeQuantum analyzes the characteristics of the underlying memory architecture to identify the optimal allocation method for edge devices.
Table~\ref{tab:memory_compat} summarizes the compatibility of memory types on the Jetson unified memory architecture.
We investigated three memory types: Device Memory (\texttt{cudaMalloc}), Pinned Host Memory (\texttt{cudaMallocHost}), and Unified Virtual Memory (\texttt{cudaMallocManaged}).

\begin{table}[t]
    \centering
    \caption{Memory type compatibility on Jetson unified memory architecture.}
    \label{tab:memory_compat}
    \begin{tabular}{lccc}
    \hline
    Memory Type & POSIX I/O & cuStateVec & Notes \\
    \hline
    Device Memory (cudaMalloc) & No & Yes & Requires explicit cudaMemcpy for I/O \\
    Pinned Host (cudaMallocHost) & Yes & No & cuStateVec rejects pointers on Tegra \\
    Unified Virtual Memory (cudaMallocManaged) & Yes & Yes & Supports zero-copy via attach modes \\
    \hline
    \end{tabular}
\end{table}

\noindent\textbf{Limitations of Conventional Allocation.}
Device memory supports \textit{cuStateVec} operations but fails to support direct POSIX I/O system calls such as \texttt{pread}.
This necessitates explicit \texttt{cudaMemcpy} calls to move data between the file system and the GPU, which introduces significant latency and doubles the memory bandwidth requirement.
Pinned host memory supports efficient I/O operations including \texttt{O\_DIRECT}.
However, our empirical analysis reveals that \textit{cuStateVec} kernels reject pinned memory pointers on Tegra architectures, preventing its use for computation.

\noindent\textbf{Adoption of UVM.}
In contrast, UVM proves to be the only memory type that supports both direct POSIX I/O and \textit{cuStateVec} execution.
On Jetson architectures, the CPU and GPU share the same physical LPDDR5 DRAM banks.
The memory controller maintains cache coherency through hardware snooping, allowing data written by the CPU via \texttt{pread} to be immediately visible to the GPU without physical data migration.
Consequently, \EdgeQuantum utilizes UVM to implement a zero-copy data path.
This approach reduces the end-to-end processing time for a 256 MB chunk by approximately 2.00$\times$ compared to the conventional pinned memory approach that requires explicit copying.

\subsection{Triple-Buffer Asynchronous Pipeline}~\label{design_3}

Although UVM enables zero-copy access, naive concurrent access by the CPU and GPU leads to resource conflicts and segmentation faults.
To address this, \EdgeQuantum implements a triple-buffer asynchronous pipeline that orchestrates memory access permissions.
Figure~\ref{fig:pipeline} illustrates the structure of the pipeline, which consists of three stages: Load, Compute, and Store.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/fig_compression_mechanism.pdf}
    \caption{Triple-buffer asynchronous pipeline (Load, Compute, Store) and UVM attach-mode transitions.}
    \label{fig:pipeline}
\end{figure}

\noindent\textbf{Dynamic Access Control.}
The core mechanism of the pipeline is the dynamic switching of UVM stream attachment.
Standard UVM usage allows the driver to manage page migration implicitly, which often results in unpredictable stalling or faults during concurrent I/O.
\EdgeQuantum creates an explicit barrier between the CPU and GPU views of memory.
We allocate three UVM buffers ($B_0, B_1, B_2$) and initialize them in \texttt{AttachHost} mode.
During the \textit{Compute} stage, \textit{Pipeline Coordinator} issues \texttt{cudaStreamAttachMemAsync} with the \texttt{AttachGlobal} flag for the specific buffer assigned to the GPU.
This operation modifies the GPU page table entries to grant access permissions without moving physical data.
Simultaneously, the other two buffers remain in \texttt{AttachHost} mode, allowing worker threads to perform \texttt{pread} and \texttt{pwrite} operations safely.

\noindent\textbf{Pipeline Scheduling.}
\EdgeQuantum schedules the processing of chunks in a round-robin fashion across the three buffers.
For a chunk $c$, the system assigns buffer $B_{i}$ where $i = c \mod 3$.
The pipeline ensures that while the GPU computes on chunk $c$ in buffer $B_i$, the I/O threads are simultaneously writing the result of chunk $c-1$ from buffer $B_{i-1}$ and reading the data for chunk $c+1$ into buffer $B_{i+1}$.
We optimize this flow by leveraging the asynchronous nature of the attach operation.
Specifically, \EdgeQuantum does not enforce a synchronization barrier after switching to \texttt{AttachGlobal}, relying on the GPU driver to implicitly wait for the permission update.
Conversely, a mandatory synchronization is enforced before switching back to \texttt{AttachHost} to ensure that all GPU writes are committed before the CPU initiates disk I/O.
This precise control enables \EdgeQuantum to hide the latency of NVMe storage behind the GPU computation time.

\subsection{Edge-Specific Optimization}~\label{design_4}

To maximize performance on resource-constrained edge devices, \EdgeQuantum applies system-level optimizations that align with the hardware characteristics of the Jetson platform.

\noindent\textbf{Bypassing Page Cache.}
Standard Linux file I/O utilizes the page cache, which introduces an extra memory copy and pollutes the limited DRAM capacity of edge devices.
For quantum simulations where the state vector size far exceeds the physical RAM, this caching behavior causes severe thrashing.
\EdgeQuantum employs the \texttt{O\_DIRECT} flag for all file operations.
This instructs the kernel to bypass the operating system's page cache and perform Direct Memory Access (DMA) transfers between the NVMe storage and the UVM buffers.
Our design ensures that the UVM buffers are properly aligned to the memory page boundaries required by \texttt{O\_DIRECT}, enabling maximum storage throughput.

\noindent\textbf{Kernel Execution Tuning.}
On embedded GPUs, the latency of kernel launching and context switching can significantly impact the pipeline efficiency.
\EdgeQuantum mitigates this by initializing high-priority CUDA streams using \texttt{cudaStreamCreateWithPriority}.
This ensures that the quantum simulation kernels preempt other background graphics or system tasks.
Additionally, we enforce a strict chunk size selection strategy.
Based on empirical testing on the Jetson Orin Nano, \EdgeQuantum defaults to a chunk size of 256 MB ($2^{25}$ amplitudes).
This size provides a balance that is large enough to saturate the GPU's compute capability while remaining small enough to fit three pipeline buffers within the 8 GB unified memory limit, leaving sufficient headroom for the \textit{cuStateVec} workspace.

\subsection{\EdgeQuantum Implementation}

We implemented \EdgeQuantum as a standalone C++ library with Python bindings for benchmarking.
The implementation modifies approximately 1,200 lines of code across five core modules: \texttt{simulator.cpp}, \texttt{chunk\_manager.cpp}, \texttt{io\_backend.cpp}, \texttt{main.cpp}, and \texttt{comprehensive\_benchmark.py}.
1) We designed a \textit{Chunk Manager} module that handles the allocation of UVM buffers with specific access flags and manages the mapping between logical state vector indices and physical NVMe offsets.
2) We implemented the \textit{IO Backend} to support \texttt{O\_DIRECT} operations, ensuring correct memory alignment and providing fallback mechanisms for incompatible filesystems.
3) We developed the main simulation loop to orchestrate the triple-buffer pipeline, integrating the dynamic \texttt{cudaStreamAttachMemAsync} logic to prevent concurrent access faults.
We opensource the code of \EdgeQuantum in the following link: \url{https://github.com/sunggonkim/IoTJ-EdgeQuantum}.